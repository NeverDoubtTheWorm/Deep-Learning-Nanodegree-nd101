
# Learning Resources
- [RNNs](#rnns)
- [LSTMs](#lstms)
- [Hyperparameters](#hyperparameters)
- [Style Transfer](#style-transfer)
- [Music and LSTMs](#music-and-lstms)
- [Text Summarization](#text-summarization)
- [Weight Initialization](#weight-initialization)
- [Language Translation](#language-translation)
- [Dynamic Memory Networks](#dynamic-memory-networks)


## RNNs
- Andrej Karpathy's [lecture on RNNs and LSTMs from CS231n](https://www.youtube.com/watch?v=iX5V1WpxxkY)
- [A great blog post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Christopher Olah on how LSTMs work.
- [Building an RNN from the ground up](http://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html), this is a little more advanced, but has an implementation in TensorFlow.

## LSTMs
- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [LSTM Networks for Sentiment Analysis](http://deeplearning.net/tutorial/lstm.html)
- [A Beginner's Guide to Recurrent Networks and LSTMs](https://deeplearning4j.org/lstm.html)
- [TensorFlow's Recurrent Neural Network Tutorial](https://www.tensorflow.org/tutorials/recurrent)
- [Time Series Prediction with LSTM Recurrent Neural Networks in Python with Keras](http://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/)
- [Demystifying LSTM neural networks](https://blog.terminal.com/demistifying-long-short-term-memory-lstm-recurrent-neural-networks/)

## Hyperparameters
- [Practical recommendations for gradient-based training of deep architectures](https://arxiv.org/abs/1206.5533) by Yoshua Bengio
- [Deep Learning book - chapter 11.4: Selecting Hyperparameters](http://www.deeplearningbook.org/contents/guidelines.html) by Ian Goodfellow, Yoshua Bengio, Aaron Courville
- [Neural Networks and Deep Learning book - Chapter 3: How to choose a neural network's hyper-parameters?](http://neuralnetworksanddeeplearning.com/chap3.html#how_to_choose_a_neural_network's_hyper-parameters) by Michael Nielsen
- [Efficient BackProp (pdf)](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf) by Yann LeCun
#### More specialized sources:
- [How to Generate a Good Word Embedding?](https://arxiv.org/abs/1507.05523) by Siwei Lai, Kang Liu, Liheng Xu, Jun Zhao
- [Systematic evaluation of CNN advances on the ImageNet](https://arxiv.org/abs/1606.02228) by Dmytro Mishkin, Nikolay Sergievskiy, Jiri Matas
- [Visualizing and Understanding Recurrent Networks](https://arxiv.org/abs/1506.02078) by Andrej Karpathy, Justin Johnson, Li Fei-Fei

## Style Transfer
- [Convolutional neural networks for artistic style transfer](https://harishnarayanan.org/writing/artistic-style-transfer/) by Harish Narayanan
- [Style transfer](https://ml4a.github.io/ml4a/style_transfer/) from Machine Learning for Artists
- [Experiments with style transfer](http://genekogan.com/works/style-transfer/) by Gene Kogan
- [A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576) by Leon A. Gatys, Alexander S. Ecker, Matthias Bethge
- [How do these "neural network style transfer" tools work?](https://jvns.ca/blog/2017/02/12/neural-style/) by Julia Evans
#### Style transfer apps:
- [Pikazo](http://www.pikazoapp.com/)
- [DeepArt.io](http://deepart.io/)
- [Artisto](https://artisto.my.com/)
- [Prisma-ai](https://prisma-ai.com/)

## Music and LSTMs
- [Understanding LSTM and its diagrams](https://medium.com/@shiyan/understanding-lstm-and-its-diagrams-37e2f46f1714#.swstv6z61) by Shi Yan
- [Sequence prediction using recurrent neural networks(LSTM) with TensorFlow](http://mourafiq.com/2016/05/15/predicting-sequences-using-rnn-in-tensorflow.html) by Mourad Mourafiq
- [A Recurrent Neural Network Music Generation Tutorial](https://magenta.tensorflow.org/2016/06/10/recurrent-neural-network-generation-tutorial/) by Dan Abolafia
- [Modeling and generating sequences of polyphonic music with the RNN-RBM](http://deeplearning.net/tutorial/rnnrbm.html) from deeplearning.net
- [Training a Recurrent Neural Network to Compose Music](https://maraoz.com/2016/02/02/abc-rnn/) by Manuel Araoz
- [Music Generation and Algorithmic Composition](http://www.cs.cmu.edu/~music//cmsip/slides/05-algo-comp.pdf) by Roger B. Dannenberg
- [Composing Music With Recurrent Neural Networks](http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/) by Daniel Johnson
- [AlgorithmicMusic Subreddit](https://www.reddit.com/r/algorithmicmusic/)

## Text Summarization
- [Has Deep Learning been applied to automatic text summarization (successfully)?](https://www.quora.com/Has-Deep-Learning-been-applied-to-automatic-text-summarization-successfully)
- [Text summarization with TensorFlow](https://research.googleblog.com/2016/08/text-summarization-with-tensorflow.html) from the Google Research Blog
- [Automatic summarization](https://en.wikipedia.org/wiki/Automatic_summarization) Wikipedia Page
- [Recurrent Neural Networks with Word Embeddings](http://deeplearning.net/tutorial/rnnslu.html) from deeplearning.net
- [Text Generation With LSTM Recurrent Neural Networks in Python with Keras](http://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/) by Jason Brownlee

## Weight Initialization
- [Understanding the difficulty of training deep feedforward neural networks](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)
- [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/pdf/1502.01852v1.pdf)
- [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167v2.pdf)

## Language Translation
- [Machine Learning is Fun Part 5: Language Translation with Deep Learning and the Magic of Sequences](https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa)
- [Sequence-to-Sequence Models](https://www.tensorflow.org/tutorials/seq2seq) from tensorflow docs
- [Introduction to Neural Machine Translation with GPUs](https://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-with-gpus/)
- [A Practical Guide to Neural Machine Translation](https://www.youtube.com/watch?v=vxibD6VaOfI) from Microsoft Research
- [Machine Translation Tutorial](http://neural-monkey.readthedocs.io/en/latest/machine_translation.html) by Jindřich Libovický, Jindřich Helcl, Tomáš Musil
- [How does Neural Machine Translation work?](http://blog.systransoft.com/how-does-neural-machine-translation-work/) from the SYSTRAN Blog
- [Attention And Memory In Deep Learning And Nlp](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp) from WildML
- [Enabling Multilingual Neural Machine Translation with TensorFlow](https://blog.altoros.com/enabling-multilingual-neural-machine-translation-with-tensorflow.html) by Sophie Turol
- [How can I build a machine translation system?](https://www.quora.com/How-can-I-build-a-machine-translation-system)
- [Attention Mechanism](https://blog.heuritech.com/2016/01/20/attention-mechanism/) by Leonard Blier
- [Peeking into the neural network architecture used for Google's Neural Machine Translation](https://smerity.com/articles/2016/google_nmt_arch.html)

## Dynamic Memory Networks
- [Dynamic Memory Networks for Visual and Textual Question Answering](https://www.youtube.com/watch?v=FCtpHt6JEI8&t=643s)
- [Dynamic Memory Networks for Visual and Textual Question Answering - Stephen Merity (MetaMind)](https://www.youtube.com/watch?v=Qf0BqEk5n3o&t=637s)
- [Implementing Dynamic memory networks](https://yerevann.github.io/2016/02/05/implementing-dynamic-memory-networks/)
- [End-To-End Memory Networks](https://www.youtube.com/watch?v=2A5DKPA5lAw)
- [Attention And Memory In Deep Learning And Nlp](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/)
- [End-To-End Memory Network using Tensorflow](https://github.com/domluna/memn2n)

